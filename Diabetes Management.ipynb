{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing all necessary libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from urllib.request import Request, urlopen\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The survey on the diabetes management with machine learning depicts that machine learning and the individuals who may have diabetes will be identified. Material was contained on medical exams, blood and urine samples, and questionnaires for patients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'diabetes_data_upload.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5ec20069a537>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfa\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'diabetes_data_upload.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1366\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1367\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1368\u001b[1;33m             \u001b[0mstorage_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1369\u001b[0m         )\n\u001b[0;32m   1370\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    645\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    646\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 647\u001b[1;33m                 \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    648\u001b[0m             )\n\u001b[0;32m    649\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'diabetes_data_upload.csv'"
     ]
    }
   ],
   "source": [
    "dfa = pd.read_csv('diabetes_data_upload.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Class' feature column is identfied as the Target Variable Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for Null Values in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(dfa.isnull(), cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the heatmap is spotless we can say there are no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mapping or Converting the text into values\n",
    "\n",
    "dfa['Gender'] = dfa['Gender'].map({'Male':1,'Female':0})\n",
    "dfa['class'] = dfa['class'].map({'Positive':1,'Negative':0})\n",
    "dfa['Polyuria'] = dfa['Polyuria'].map({'Yes':1,'No':0})\n",
    "dfa['Polydipsia'] = dfa['Polydipsia'].map({'Yes':1,'No':0})\n",
    "dfa['sudden weight loss'] = dfa['sudden weight loss'].map({'Yes':1,'No':0})\n",
    "dfa['weakness'] = dfa['weakness'].map({'Yes':1,'No':0})\n",
    "dfa['Polyphagia'] = dfa['Polyphagia'].map({'Yes':1,'No':0})\n",
    "dfa['Genital thrush'] = dfa['Genital thrush'].map({'Yes':1,'No':0})\n",
    "dfa['visual blurring'] = dfa['visual blurring'].map({'Yes':1,'No':0})\n",
    "dfa['Itching'] = dfa['Itching'].map({'Yes':1,'No':0})\n",
    "dfa['Irritability'] = dfa['Irritability'].map({'Yes':1,'No':0})\n",
    "dfa['delayed healing'] = dfa['delayed healing'].map({'Yes':1,'No':0})\n",
    "dfa['partial paresis'] = dfa['partial paresis'].map({'Yes':1,'No':0})\n",
    "dfa['muscle stiffness'] = dfa['muscle stiffness'].map({'Yes':1,'No':0})\n",
    "dfa['Alopecia'] = dfa['Alopecia'].map({'Yes':1,'No':0})\n",
    "dfa['Obesity'] = dfa['Obesity'].map({'Yes':1,'No':0})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determining the correlation between independent and dependent variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The independent variables that has high correlation with the dependent variables and less correlation with other variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically we check how much eaach parameter or features given in the dataset affects our target result 'Class'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrdata = dfa.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax,fig = plt.subplots(figsize=(15,8))\n",
    "sns.heatmap(corrdata,annot=True,cmap=\"YlGnBu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The Age Distribution from the dataset\n",
    "sns.distplot(dfa['Age'],bins=30)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Age/class(dependent variable)\n",
    "\n",
    "sns.barplot(x='class',y='Age',data=dfa)\n",
    "ds = dfa['class'].value_counts().reset_index()\n",
    "ds.columns = ['class', 'count']\n",
    "plot=ds.plot.pie(y='count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gender Distribution Plot\n",
    "sns.countplot(x='class',data=dfa,hue='Gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polyuria\n",
    "\n",
    "sns.catplot(x=\"Polyuria\", y=\"class\", kind=\"point\", data=dfa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polydipsia\n",
    "\n",
    "sns.barplot(x='Polydipsia',y='class',data=dfa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sudden weight loss\n",
    "\n",
    "sns.countplot(x='class',data=dfa,hue='sudden weight loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Polyphagia\n",
    "\n",
    "sns.countplot(x='class',data=dfa, hue='Polyphagia')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Genital Thrush\n",
    "\n",
    "sns.catplot(x='class',y='Genital thrush',kind='point',data=dfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Partial paresis\n",
    "\n",
    "sns.barplot(x='class',y='partial paresis',data=dfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='class',y='Alopecia',data=dfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visual blurring\n",
    "\n",
    "sns.barplot(x=\"visual blurring\", y=\"class\", data=dfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Itching\n",
    "\n",
    "sns.barplot(x=\"Itching\", y=\"class\", data=dfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obesity\n",
    "\n",
    "sns.barplot(x='class',y='Obesity',data=dfa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Irritability\n",
    "\n",
    "sns.barplot(x='Irritability',y='class',data=dfa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Separating the dataframe into their independent and dependent variable categories for determination of the best features to be used for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = dfa.iloc[:,0:-1]\n",
    "y1 = dfa.iloc[:,-1]\n",
    "\n",
    "X1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection using sklearn's KBest and Chi-Square test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "best_feature = SelectKBest(score_func=chi2,k=10)\n",
    "fit = best_feature.fit(X1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_scores = pd.DataFrame(fit.scores_)\n",
    "dataset_cols = pd.DataFrame(X1.columns)\n",
    "featurescores = pd.concat([dataset_cols,dataset_scores],axis=1)\n",
    "featurescores.columns=['column','scores']\n",
    "\n",
    "featurescores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above are the features and their scores according to their significance to the target to be determined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureview=pd.Series(fit.scores_, index=X1.columns)\n",
    "featureview.plot(kind='barh')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here we need to select the top 10 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(featurescores.nlargest(10,'scores'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the variance of each feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features must have high variance with other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "feature_high_variance = VarianceThreshold(threshold=(0.5*(1-0.5)))\n",
    "falls=feature_high_variance.fit(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfa_scores1 = pd.DataFrame(falls.variances_)\n",
    "dat1 = pd.DataFrame(X1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_variance = pd.concat([dfa_scores1,dat1],axis=1)\n",
    "high_variance.columns=['variance','cols']\n",
    "high_variance[high_variance['variance']>0.2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfa[['Polydipsia','sudden weight loss','partial paresis','Irritability','Polyphagia','Age','visual blurring']]\n",
    "y = dfa['class']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data into Training and Test sets and Standardizing the values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2,random_state=5)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "ss = StandardScaler()\n",
    "X_train = ss.fit_transform(X_train)\n",
    "X_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lg=LogisticRegression()\n",
    "lg.fit(X_train,y_train)\n",
    "\n",
    "#Cross validation test for training data\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator=lg, X=X_train ,y=y_train,cv=10)\n",
    "print(\"accuracy is {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"std is {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction Below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre=lg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the accuracy and confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression=accuracy_score(pre,y_test)\n",
    "print(accuracy_score(pre,y_test))\n",
    "print(confusion_matrix(pre,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, Logistic Regression test results in accuracy of 0.913"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "sv=SVC(kernel='linear',random_state=0)\n",
    "sv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator=sv, X=X_train ,y=y_train,cv=10)\n",
    "print(\"accuracy is {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"std is {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre1=sv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_linear=accuracy_score(pre1,y_test)\n",
    "print(accuracy_score(pre1,y_test))\n",
    "print(confusion_matrix(pre1,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(pre1,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svrf=SVC(kernel='rbf',random_state=0)\n",
    "svrf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator=svrf, X=X_train ,y=y_train,cv=10)\n",
    "print(\"accuracy is {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"std is {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre2=svrf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_rbf=accuracy_score(pre2,y_test)\n",
    "print(accuracy_score(pre2,y_test))\n",
    "print(confusion_matrix(pre2,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "score=[]\n",
    "\n",
    "for i in range(1,10):\n",
    "    knn=KNeighborsClassifier(n_neighbors=i,metric='minkowski',p=2)\n",
    "    knn.fit(X_train,y_train)\n",
    "    pre3=knn.predict(X_test)\n",
    "    ans=accuracy_score(pre3,y_test)\n",
    "    score.append(round(100*ans,2))\n",
    "print(sorted(score,reverse=True)[:5])\n",
    "knn=sorted(score,reverse=True)[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian networks are a probabilistic model through which it is feasible to construct a graph between the causes of an event - independent variables and its consequences - dependent variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method would be able to classify a new set of independent variables and decide which class they belong to depending on the statistical function that produces the highest value using these specified functions. In the case of diseases, the system operates from a series of simple parameters taken from the patient and does not require laboratory analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gb=GaussianNB()\n",
    "gb.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator=gb, X=X_train ,y=y_train,cv=10)\n",
    "print(\"accuracy is {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"std is {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre4=gb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Naive_bayes_Gaussian_nb=accuracy_score(pre4,y_test)\n",
    "print(accuracy_score(pre4,y_test))\n",
    "print(confusion_matrix(pre4,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dc=DecisionTreeClassifier(criterion='gini')\n",
    "dc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator=dc, X=X_train ,y=y_train,cv=10)\n",
    "print(\"accuracy is {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"std is {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre5=dc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Decisiontrees_classifier=accuracy_score(pre5,y_test)\n",
    "print(accuracy_score(pre5,y_test))\n",
    "print(confusion_matrix(pre5,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "estime=[]\n",
    "for i in range(1,100):\n",
    "    rc=RandomForestClassifier(n_estimators=i,criterion='entropy',random_state=0)\n",
    "    rc.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "accuracies = cross_val_score(estimator=rc, X=X_train ,y=y_train,cv=10)\n",
    "print(\"accuracy is {:.2f} %\".format(accuracies.mean()*100))\n",
    "print(\"std is {:.2f} %\".format(accuracies.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre6 = rc.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random_forest=accuracy_score(pre6,y_test)\n",
    "print(accuracy_score(pre6,y_test))\n",
    "print(confusion_matrix(pre6,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Logistic regression:',logistic_regression)\n",
    "print('svmlinear:',svm_linear)\n",
    "print('svmrbf:',svm_rbf)\n",
    "print('knn:',knn)\n",
    "print('naive bayes:',Naive_bayes_Gaussian_nb)\n",
    "print('Decision tress:',Decisiontrees_classifier)\n",
    "print('Random forest:',Random_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Best Models are KNN, Decision Tree and Random Forest with 96.15 % score followed by KNN and Decision tree with 96.15% accuracy and Random forest with 95.19% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patient Readmission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine learning techniques allow to automatically identify patterns and even make predictions based on a large amount of data that could be extracted from the computer systems used to ascertain information on readmission of diabetes patients. The analysis Clustering or grouping is a technique that allows exploring a setoff objects to determine if there are groups that can be significantly represented by certain characteristics, in this way, objects of the same group are very similar to each other and different from objects in other groups \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file and create a pandas dataframe\n",
    "data = pd.read_csv('diabetic_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimensions of the data\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a first look at the dataset\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first 5 rows of data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the response variable 'readmitted' in the original dataset contains three categories.\n",
    "# 11% of patients were readmitted within 30 days (<30)\n",
    "# 35% of patients were readmitted after 30 days (>30)\n",
    "# 54% of patients were never readmitted (NO)\n",
    "data.groupby('readmitted').size().plot(kind='bar')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recode the response variable so that it becomes a binary classification task.\n",
    "# '0' means no readmission\n",
    "# '1' means readmission no matter how many days after being discharged\n",
    "data['readmitted'] = pd.Series([0 if val == 'NO' else 1 for val in data['readmitted']])\n",
    "data_origin = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot 'readmitted' again to see the change\n",
    "data.groupby('readmitted').size().plot(kind='bar')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove irrelevant features\n",
    "data.drop(['encounter_id', 'patient_nbr', 'payer_code'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check NA in 'weight'\n",
    "data[data['weight'] == '?'].shape[0] * 1.0 / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check NA in 'medical_specialty'\n",
    "data[data['medical_specialty'] == '?'].shape[0] * 1.0 / data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'weight' and 'medical_specialty' because it's hard to do imputation on them\n",
    "data.drop(['weight', 'medical_specialty'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove rows that have NA in 'race', 'diag_1', 'diag_2', or 'diag_3'\n",
    "# remove rows that have invalid values in 'gender'\n",
    "data = data[data['race'] != '?']\n",
    "data = data[data['diag_1'] != '?']\n",
    "data = data[data['diag_2'] != '?']\n",
    "data = data[data['diag_3'] != '?']\n",
    "data = data[data['gender'] != 'Unknown/Invalid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check 'age' feature\n",
    "data.groupby('age').size().plot(kind='bar')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recategorize 'age' so that the population is more evenly distributed\n",
    "data['age'] = pd.Series(['[0-50)' if val in ['[0-10)', '[10-20)', '[20-30)', '[30-40)', '[40-50)'] else val \n",
    "                         for val in data['age']], index=data.index)\n",
    "data['age'] = pd.Series(['[80-100)' if val in ['[80-90)', '[90-100)'] else val \n",
    "                         for val in data['age']], index=data.index)\n",
    "\n",
    "data.groupby('age').size().plot(kind='bar')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original 'discharge_disposition_id' contains 28 levels\n",
    "# reduce 'discharge_disposition_id' levels into 2 categories\n",
    "# discharge_disposition_id = 1 corresponds to 'Discharge Home'\n",
    "data['discharge_disposition_id'] = pd.Series(['Home' if val == 1 else 'Other discharge' \n",
    "                                              for val in data['discharge_disposition_id']], index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original 'admission_source_id' contains 25 levels\n",
    "# reduce 'admission_source_id' into 3 categories\n",
    "data['admission_source_id'] = pd.Series(['Emergency Room' if val == 7 else 'Referral' if val == 1 else 'Other source' \n",
    "                                              for val in data['admission_source_id']], index=data.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original 'admission_type_id' contains 8 levels\n",
    "# reduce 'admission_type_id' into 2 categories\n",
    "data['admission_type_id'] = pd.Series(['Emergency' if val == 1 else 'Other type' \n",
    "                                              for val in data['admission_type_id']], index=data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23 features represent different medications.So, it has to be decided should we use them all in the model?\n",
    "\n",
    "How to deal with 'diag_1', 'diag_2', and 'diag_3'?\n",
    "\n",
    "Based on the results found in Reference[1], we know that:\n",
    "\n",
    "    The statistical model suggests that the relationship between the probability of readmission and the HbA1c measurement depends on the primary diagnosis.\n",
    "\n",
    "    The only medication that varied significantly across the patients was the delivery of insulin, while other medications remained common among all the patients.\n",
    "\n",
    "    It may not be surprising that the attention given to diabetes care in individuals with admitting diagnoses of circulatory or respiratory diseases may have been less than those with a primary diagnosis of diabetes mellitus.\n",
    "\n",
    "    Greater attention to diabetes care during the hospitalization for these high-risk individuals may have a significant impact on readmission.\n",
    "\n",
    "    The primary, secondary, and third medical diagnoses are marked by the ICD9 codes.\n",
    "\n",
    "    ICD9 code for diabetes: 250.xx\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare diabetes medications 'miglitol', 'nateglinide' and 'acarbose' with 'insulin', as an example\n",
    "fig = plt.figure(figsize=(20,15))\n",
    "\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax1 = data.groupby('miglitol').size().plot(kind='bar')\n",
    "plt.xlabel('miglitol', fontsize=15)\n",
    "plt.ylabel('Count', fontsize=15)\n",
    "\n",
    "ax2 = fig.add_subplot(222)\n",
    "ax2 = data.groupby('nateglinide').size().plot(kind='bar')\n",
    "plt.xlabel('nateglinide', fontsize=15)\n",
    "plt.ylabel('Count', fontsize=15)\n",
    "\n",
    "ax3 = fig.add_subplot(223)\n",
    "ax3 = data.groupby('acarbose').size().plot(kind='bar')\n",
    "plt.xlabel('acarbose', fontsize=15)\n",
    "plt.ylabel('Count', fontsize=15)\n",
    "\n",
    "ax4 = fig.add_subplot(224)\n",
    "ax4 = data.groupby('insulin').size().plot(kind='bar')\n",
    "plt.xlabel('insulin', fontsize=15)\n",
    "plt.ylabel('Count', fontsize=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only 'insulin' and remove the other 22 diabetes medications\n",
    "data.drop(['metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', \n",
    "           'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', \n",
    "           'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'examide', \n",
    "           'citoglipton', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone',\n",
    "           'metformin-rosiglitazone', 'metformin-pioglitazone'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# denote 'diag_1' as '1' if it relates to diabetes and '0' if it's not\n",
    "# remove 'diag_2' and 'diag_3'\n",
    "data['diag_1'] = pd.Series([1 if val.startswith('250') else 0 for val in data['diag_1']], index=data.index)\n",
    "data.drop(['diag_2', 'diag_3'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we end up with 98052 rows, 20 features, and one response variable 'readmitted' \n",
    "list(data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot-encoding on categorical features\n",
    "# convert nominal values to dummy values\n",
    "df_age = pd.get_dummies(data['age'])\n",
    "df_race = pd.get_dummies(data['race'])\n",
    "df_gender = pd.get_dummies(data['gender'])\n",
    "df_max_glu_serum = pd.get_dummies(data['max_glu_serum'])\n",
    "df_A1Cresult = pd.get_dummies(data['A1Cresult'])\n",
    "df_insulin = pd.get_dummies(data['insulin'])\n",
    "df_change = pd.get_dummies(data['change'])\n",
    "df_diabetesMed = pd.get_dummies(data['diabetesMed'])\n",
    "df_discharge_disposition_id = pd.get_dummies(data['discharge_disposition_id'])\n",
    "df_admission_source_id = pd.get_dummies(data['admission_source_id'])\n",
    "df_admission_type_id = pd.get_dummies(data['admission_type_id'])\n",
    "\n",
    "data = pd.concat([data, df_age, df_race, df_gender, df_max_glu_serum, df_A1Cresult, \n",
    "                  df_insulin, df_change, df_diabetesMed, df_discharge_disposition_id, \n",
    "                  df_admission_source_id, df_admission_type_id], axis=1)\n",
    "data.drop(['age', 'race', 'gender', 'max_glu_serum', 'A1Cresult', 'insulin', 'change', \n",
    "                  'diabetesMed', 'discharge_disposition_id', 'admission_source_id', \n",
    "                  'admission_type_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply square root transformation on right skewed count data to reduce the effects of extreme values.\n",
    "# here log transformation is not appropriate because the data is Poisson distributed and contains many zero values.\n",
    "data['number_outpatient'] = data['number_outpatient'].apply(lambda x: np.sqrt(x + 0.5))\n",
    "data['number_emergency'] = data['number_emergency'].apply(lambda x: np.sqrt(x + 0.5))\n",
    "data['number_inpatient'] = data['number_inpatient'].apply(lambda x: np.sqrt(x + 0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature scaling, features are standardized to have zero mean and unit variance\n",
    "feature_scale_cols = ['time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', \n",
    "                      'number_diagnoses', 'number_inpatient', 'number_emergency', 'number_outpatient']\n",
    "\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler().fit(data[feature_scale_cols])\n",
    "data_scaler = scaler.transform(data[feature_scale_cols])\n",
    "\n",
    "data_scaler_df = pd.DataFrame(data=data_scaler, columns=feature_scale_cols, index=data.index)\n",
    "data.drop(feature_scale_cols, axis=1, inplace=True)\n",
    "data = pd.concat([data, data_scaler_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation Summary\n",
    "\n",
    "    Remove NA in 'race', 'gender', 'diag_1', 'diag_2', and 'diag_3'\n",
    "    Remove 29 features in total: 'encounter_id', 'patient_nbr', 'payer_code', 'weight', 'medical_specialty', 'diag_2', 'diag_3', and 22 features for medications, including 'metformin', 'repaglinide', 'nateglinide', 'chlorpropamide', 'glimepiride', 'acetohexamide', 'glipizide', 'glyburide', 'tolbutamide', 'pioglitazone', 'rosiglitazone', 'acarbose', 'miglitol', 'troglitazone', 'tolazamide', 'examide', 'citoglipton', 'glyburide-metformin', 'glipizide-metformin', 'glimepiride-pioglitazone', 'metformin-rosiglitazone', and 'metformin-pioglitazone'\n",
    "    Recategorize 'age' feature\n",
    "    Reduce levels in 'discharge_disposition_id', 'admission_source_id', and 'admission_type_id'\n",
    "    One-hot-encode on categorical data\n",
    "    Square root transform on right skewed count data\n",
    "    Apply feature standardizing on numerical data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the importance of different features by using emsemble method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X (features) and y (response)\n",
    "X = data.drop(['readmitted'], axis=1)\n",
    "y = data['readmitted']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into cross-validation (75%) and testing (25%) data sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_cv, X_test, y_cv, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit Random Forest model to the cross-validation data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "forest = RandomForestClassifier()\n",
    "forest.fit(X_cv, y_cv)\n",
    "importances = forest.feature_importances_\n",
    "\n",
    "# make importance relative to the max importance\n",
    "feature_importance = 100.0 * (importances / importances.max())\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "feature_names = list(X_cv.columns.values)\n",
    "feature_names_sort = [feature_names[indice] for indice in sorted_idx]\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "print ('Top 6 features are: ')\n",
    "for feature in feature_names_sort[::-1][:6]:\n",
    "    print (feature)\n",
    "\n",
    "# plot the result\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, feature_names_sort)\n",
    "plt.title('Relative Feature Importance', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a smaller feature set which only contains the top 6 features\n",
    "X_cv_top6 = X_cv[['time_in_hospital', 'num_lab_procedures', 'num_procedures', 'num_medications', 'number_diagnoses',\n",
    "           'number_inpatient']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the best model using 10-fold cross validation. The metric that will be using is the accuracy of prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf1 = RandomForestClassifier()\n",
    "RF_score = cross_val_score(clf1, X_cv, y_cv, cv=10, scoring='accuracy').mean()\n",
    "RF_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf2 = GaussianNB()\n",
    "NB_score = cross_val_score(clf2, X_cv, y_cv, cv=10, scoring='accuracy').mean()\n",
    "NB_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "clf3 = LogisticRegression()\n",
    "LR_score = cross_val_score(clf3, X_cv, y_cv, cv=10, scoring='accuracy').mean()\n",
    "LR_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot and compare the scores\n",
    "# LR outperforms the other two a little bit\n",
    "x_axis = np.arange(3)\n",
    "y_axis = [RF_score, NB_score, LR_score]\n",
    "plt.bar(x_axis, y_axis, width=0.4)\n",
    "plt.xticks(x_axis + 0.4/2., ('Random Forest', 'Naive bayes', 'Logistic Regression'))\n",
    "plt.ylabel('Cross-Validated Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression on Top 6 features\n",
    "# still be able to achieve good result with reduced running time\n",
    "LR_score_top = cross_val_score(clf3, X_cv_top6, y_cv, cv=10, scoring='accuracy').mean()\n",
    "LR_score_top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tuning Using GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use GridSearchCV to tune parameter C in logistic regression. Smaller C values specify stronger regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter values that should be searched\n",
    "C_range = np.arange(0.1, 3.1, 0.2)\n",
    "param_grid = dict(C = C_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid, fit the grid with data\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = LogisticRegression()\n",
    "grid = GridSearchCV(clf, param_grid, cv=10, scoring='accuracy')\n",
    "grid.fit(X_cv, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the best model\n",
    "print (grid.best_score_)\n",
    "print (grid.best_params_)\n",
    "print (grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification accuracy\n",
    "\n",
    "NULL accuracy: accuracy that could be achieved by always predicting the most frequent class -- 'No Readmission'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the class distribution of the testing set\n",
    "y_test.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate null accuracy (for binary classification problems coded as 0/1)\n",
    "max(y_test.mean(), 1 - y_test.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a logistic regression model on the cross-validation set with the optimal C parameter\n",
    "logreg = LogisticRegression(C=grid.best_params_['C'])\n",
    "logreg.fit(X_cv, y_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class predictions for the testing set \n",
    "# calculate accuracy and compare with null accuracy\n",
    "from sklearn import metrics\n",
    "y_pred_class = logreg.predict(X_test)\n",
    "print (metrics.accuracy_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Table that describes the performance of a classification model.\n",
    "\n",
    "True Positives (TP): we correctly predicted that they do have diabetes\n",
    "\n",
    "True Negatives (TN): we correctly predicted that they don't have diabetes\n",
    "\n",
    "False Positives (FP): we incorrectly predicted that they do have diabetes (a \"Type I error\")\n",
    "\n",
    "False Negatives (FN): we incorrectly predicted that they don't have diabetes (a \"Type II error\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion = metrics.confusion_matrix(y_test, y_pred_class)\n",
    "TP = confusion[1, 1]\n",
    "TN = confusion[0, 0]\n",
    "FP = confusion[0, 1]\n",
    "FN = confusion[1, 0]\n",
    "print (confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sensitivity (Recall): How \"sensitive\" is the classifier to detecting positive instances? TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.recall_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specificity: TN/(TN+FP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (TN / float(TN + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "False Positive Rate (1 - specificity): FP/(TN+FP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (FP / float(TN + FP))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Precision: TP/(TP+FP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.precision_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "F1 score: F1 = 2 x precision x recall / (precision + recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (metrics.f1_score(y_test, y_pred_class))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Receiver operating characteristic (ROC) curve and area under the curve (AUC)\n",
    "\n",
    "ROC is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied. AUC is the percentage of the ROC plot that is underneath the curve. AUC is useful as a single number summary of classifier performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1\n",
    "y_pred_prob = logreg.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot ROC curve\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred_prob)\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.title('ROC curve for diabetes readmission')\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUC score\n",
    "print (metrics.roc_auc_score(y_test, y_pred_prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjusting the classification threshold\n",
    "\n",
    "Decrease the threshold to increase the sensitivity of the classifier, which is more favorable to the prediction of diabetes patient readmission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function that accepts a threshold and prints sensitivity and specificity\n",
    "def evaluate_threshold(threshold):\n",
    "    print ('Sensitivity:', tpr[thresholds > threshold][-1])\n",
    "    print ('Specificity:', 1 - fpr[thresholds > threshold][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict readmission if the predicted probability is greater than 0.40\n",
    "# the resulting sensitivity can achieve above 0.70\n",
    "evaluate_threshold(0.40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new prediction accuracy\n",
    "y_pred_prob = logreg.predict_proba(X_test)\n",
    "y_pred_new = []\n",
    "\n",
    "for i in range(len(y_pred_prob)):\n",
    "    if y_pred_prob[i][1] > 0.4:\n",
    "        y_pred_new.append(1)\n",
    "    else:\n",
    "        y_pred_new.append(0)\n",
    "\n",
    "print (metrics.accuracy_score(y_test, y_pred_new))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "Six major features are found to have high impact on diabetes patient readmission: number of lab procedures, number of medications administrated during the encounter, time spent in hospital, number of procedures other than lab tests, number of diagnoses, and number of inpatient visits.\n",
    "\n",
    "The logistic regression classifier modeling achieves 0.62 accuracy and 0.66 AUC score. The sensitivity of the modeling can be increased by adjusting the classification threshold.\n",
    "\n",
    "To correctly predict the readmission and avoid extra cost, hospitals should carefully examine the clinical data of patients and pay special attention to the above major features.\n",
    "\n",
    "Some other features might be worth collecting, for example, date of admission and family history.\n",
    "\n",
    "This analytic method can be applied to different diseases other than diabetes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
