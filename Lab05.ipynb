{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 05 &mdash; $\\chi^2$-test and non-parametric methods\n",
    "\n",
    "## Roadmap\n",
    "\n",
    "* [$\\chi^2$-test for independence](#%24\\chi^2%24-test-for-independence)\n",
    "    * [Interpretation](#Interpretation)\n",
    "* [Testing goodness of fit](#Testing-goodness-of-fit)\n",
    "    * [$\\chi^2$-test for GoF](#chi2-test-for-GoF)\n",
    "    * [Kolmogorov-Smirnov test](#Kolmogorov-Smirnov-test)\n",
    "* [Sign test](#Sign-test)\n",
    "* [U test](#U-test)\n",
    "* [Bootstrap](#Bootstrap)\n",
    "    * [Bootstrap confidence intervals](#Bootstrap-confidence-intervals)\n",
    "    * [Bootstrap p-values](#Bootstrap-p-values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\chi^2$-test for independence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will apply the $\\chi^2$-test for independence on a case study of A/B testing from <a href=\"https://www.datacamp.com/projects/184\">Datacamp</a>. This section is adapted from [this notebook](https://www.kaggle.com/yufengsui/datacamp-project-mobile-games-a-b-testing). \n",
    "The dataset contains results on A/B testing for a mobile game called <a href=\"https://www.facebook.com/cookiecatsgame\">Cookie Cats</a>, which is a popular mobile puzzle game developed by <a href=\"http://tactile.dk\">Tactile Entertainment</a>.\n",
    "\n",
    "It's a classic \"connect three\"-style puzzle game where the player must connect tiles of the same color to clear the board and win the level. It also features singing cats. Check out this short demo:</p>\n",
    "<p><a href=\"https://youtu.be/GaP5f0jVTWE\"><img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_184/img/cookie_cats_video.jpeg\" style=\"width: 500px\"></a></p>\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As players progress through the levels of the game, they will **occasionally encounter gates that force them to wait a non-trivial amount of time or make an in-app purchase to progress**. In addition to driving in-app purchases, these gates serve the important purpose of giving players an enforced break from playing the game, hopefully resulting in that the player's enjoyment of the game being increased and prolonged.\n",
    "\n",
    "But where should the gates be placed? Initially the first gate was placed at level 30. **We're going to analyze an AB-test where we moved the first gate in Cookie Cats from level 30 to level 40.**\n",
    "\n",
    "<p><img src=\"https://s3.amazonaws.com/assets.datacamp.com/production/project_184/img/cc_gates.png\" style=\"width: 500px\"></p>\n",
    "\n",
    "In particular, we will use a $\\chi^2$ test to evaluate the impact of the gate location on player retention. The data contains two categorical variables:\n",
    "* `retention_1`: did the player come back and play 1 day after installing?\n",
    "* `retention_7`: did the player come back and play 7 days after installing?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's load the dataset and print a preview\n",
    "# available at https://moodle.royalholloway.ac.uk/mod/resource/view.php?id=455845\n",
    "df_cookie = pd.read_csv(\"cookie_cats.csv\")\n",
    "df_cookie.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now we want to establish whether the gate location has a significant effect on the 1-day retention rate\n",
    "* We first build a contingency table where the input factor is the version of the game, and the output is the 1-day retention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie_1_contable = pd.crosstab(index=df_cookie.version, columns=df_cookie.retention_1)\n",
    "cookie_1_contable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply `scipy.stats` function `chi2_contingency` which takes in input a 2-d array (a contingency table) and returns the value of the test statistic, p-value, degrees of freedom, and array of expected counts under the null (independence). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter correction applies Yatesâ€™ correction for continuity. \n",
    "# we set it to false here\n",
    "chi2, p_val, dof, exp_counts = stats.chi2_contingency(cookie_1_contable, correction=False)\n",
    "print(\"the value of the test statistic is \"+str(chi2)+\". p value is: \"+str(p_val))\n",
    "print(\"Degrees of freedom: \"+str(dof))\n",
    "print(\"Expected counts under the null: \")\n",
    "print(exp_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We didn't obtain a very small p-value ($p>0.05$), so there might not be a real effect on the 1-day retention rate. I.e., we don't have enough evidence to reject the null hypothesis that `version` and `retention_1` are independent \n",
    "* However, since players have only been playing the game for one day, it is likely that most players haven't reached level 30 yet. That is, many players won't have been affected by the gate, even if it's as early as level 30. \n",
    "* So let's now evaluate the effect on the 7-day retention rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cookie_7_contable = pd.crosstab(index=df_cookie.version, columns=df_cookie.retention_7)\n",
    "cookie_7_contable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2_7, p_val_7, dof_7, exp_counts_7 = stats.chi2_contingency(cookie_7_contable, correction=False)\n",
    "print(\"For the 7-day retention rate, value of the test statistic is \"+str(chi2_7)+\". p value is: \"+str(p_val_7))\n",
    "print(\"Degrees of freedom: \"+str(dof_7))\n",
    "print(\"Expected counts under the null: \")\n",
    "print(exp_counts_7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the difference is significant ($p\\approx 0.00155$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `statsmodel` library also offers methods to analyze contingency tables through the `Table` class. See more at https://www.statsmodels.org/stable/generated/statsmodels.stats.contingency_tables.Table.html\n",
    "* It contains attributes for the expected counts and standardized residuals, among others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a Table object giving in input the contingency table (a 2-d array)\n",
    "sm_table = sm.stats.Table(cookie_7_contable)\n",
    "# attribute fittedvalues gives the expected count under the null\n",
    "print(\"Expected counts:\")\n",
    "print(sm_table.fittedvalues)\n",
    "# attribute chi2_contribs gives the contribution of each cell to the chi-square statistic\n",
    "print(\"Chi-square statistic contributions:\")\n",
    "print(sm_table.chi2_contribs)\n",
    "# attribute resid_pearson gives the standardized residuals\n",
    "print(\"Standardized residuals:\")\n",
    "print(sm_table.resid_pearson)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform the actual $\\chi^2$-test, we invoke the `test_nominal_association` function of the `Table` object. It returns a tuple with attributes `df`, `pvalue` and `statistic`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rslt = sm_table.test_nominal_association()\n",
    "print(rslt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that there is a significant association between `gate` and `retention_7`. Let's quantify the efffect by computing manually Cramer's $V$ for both 1-day and 7-day cases. Recall that this is equals to\n",
    "$$V = \\sqrt{\\frac{\\chi^2}{\\min(r-1,c-1)\\cdot n}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define a function that takes in input the contingency table (2-d array)\n",
    "# and does all the rest\n",
    "def Cramer_V(cont_table):\n",
    "    # total number of observations\n",
    "    n = np.sum(np.sum(cont_table))\n",
    "    # number of rows and columns\n",
    "    r = cont_table.shape[0]\n",
    "    c = cont_table.shape[1]\n",
    "    # compute chi-square statistic, via e.g. scipy.stats\n",
    "    chi2_stat, _, _, _ = stats.chi2_contingency(cont_table, correction=False)\n",
    "    return np.sqrt(chi2_stat/(np.min([r-1,c-1])*n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Cramer's V for the 1-day case is: \"+str(Cramer_V(cookie_1_contable)))\n",
    "print(\"Cramer's V for the 7-day case is: \"+str(Cramer_V(cookie_7_contable)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since `statsmodels`' `Table` provides us with easy-to-use standardized residuals, we can perform ___post-hoc Z tests___ on the residuals to see which individual cell exhibit a significant difference from the expected count.\n",
    "* Recall that standardized residuals are already Z values, so we need to compute the p-values \"manually\" (because `statsmodels`'s `ztest` function requires in input the samples)\n",
    "* and we need to apply some correction for multiple tests of course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's perform post-hoc two-sided Z tests for the cells in the 2nd column \n",
    "# of sm_table (i.e., the Table object for the retain_7 case)\n",
    "\n",
    "# overall (family-wise) significance is alpha\n",
    "alpha = 0.05\n",
    "\n",
    "# standardized residuals\n",
    "std_res=sm_table.resid_pearson\n",
    "print(\"Standardized residuals (only 2nd column):\")\n",
    "print(std_res.iloc[:,1])\n",
    "\n",
    "# we have two ways here:\n",
    "# 1. compare the Z values (std. residuals) with the appropriate critical Z values (as done in lectures)\n",
    "# 2. compute p values and correct them later via sm.stats.multipletests\n",
    "\n",
    "#Â method 1.\n",
    "# corrected alpha (via Bonferroni) is alpha/num_tests. \n",
    "# here we test only the cells in the second column, so num_tests=num_rows\n",
    "# then, since it's a two-sided test, we need to divide by two the corrected significance\n",
    "n_rows = std_res.shape[0]\n",
    "alpha_cor = alpha/n_rows\n",
    "\n",
    "# critical value at alpha_cor/2 (recall, we use the ppf method of Z distribution)\n",
    "z_critical = stats.norm.ppf(alpha_cor/2)\n",
    "print(\"Critical value: \"+str(z_critical))\n",
    "\n",
    "# the above will be negative, but we take absolute values below, so we don't care\n",
    "# we compare each residual with the above. \n",
    "# If above the critical value, difference is significant \n",
    "print(np.abs(std_res.iloc[:,1])>=np.abs(z_critical))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a very interesting example showing that even though the $\\chi^2$-test establishes a statistically significant result, the individual counts are not significantly different from the expected counts. With such results, we need to be careful with our claims!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 2.\n",
    "# compute the two-sided p-values\n",
    "# in the code below, we take the min between left-tailed and right-tailed probabilities, as usual\n",
    "pvalues = 2*np.min([stats.norm.cdf(std_res.iloc[:,1]), 1-stats.norm.cdf(std_res.iloc[:,1])], axis=0)\n",
    "print(\"uncorrected p-values: \"+str(pvalues))\n",
    "# use bonferroni correction within sm.stats.multipletests function \n",
    "# (other correction  methods are possible)\n",
    "rejected, correct_pvals, _, _ = sm.stats.multipletests(pvalues, alpha=alpha, method='bonferroni')\n",
    "print(\"tests rejected? \"+str(rejected))\n",
    "print(\"corrected p-values: \"+str(correct_pvals))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing goodness-of-fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### chi2-test for GoF\n",
    "* We saw that the $\\chi^2$-test for independence can be adapted to compare an empirical discrete distribution with an a-priori model\n",
    "* `scipy.stats` offers a method `chisquare` for doing exactly that. It takes in input two parameters `f_obs` (observed counts) and `f_exp` (expected counts, those under the a priori model)\n",
    "* Let's make an experiment where we generate the observations from the a priori model, some binomial, manipulate the observations and check whether or not the manipulated dataset still fits well the original model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the a-priori model, a binomial with parameters n=10 and p=0.3\n",
    "n = 10\n",
    "apriori_dist = stats.binom(n=n,p=0.3)\n",
    "# the expected frequencies are given by the pmf of apriori_dist \n",
    "exp_frequencies = [apriori_dist.pmf(k) for k in range(n+1)]\n",
    "print(\"Expected frequencies: \"+str(exp_frequencies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a random sample of size 1000 from this binomial\n",
    "sample_size=1000\n",
    "my_sample = apriori_dist.rvs(sample_size)\n",
    "# manipulate the sample, by adding a uniform integer between -2 and 2 (and clipping to 0-10)\n",
    "low = -2\n",
    "high = 2\n",
    "my_sample = my_sample + stats.randint(low,high+1).rvs(np.shape(my_sample))\n",
    "# clip the result\n",
    "my_sample[my_sample>10]=10\n",
    "my_sample[my_sample<0]=0\n",
    "\n",
    "# let's now compute the corresponding observed frequencies\n",
    "obs_frequencies = [np.mean(my_sample==k) for k in range(n+1)]\n",
    "print(\"the observed frequencies (of the manipulated) sample are:\")\n",
    "print(obs_frequencies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, let's apply the chi-square test for GoF\n",
    "# here we need to use expected and observed counts, \n",
    "# which we simply obtain by multiplying the frequencies by 1000 (the size of our sample)\n",
    "obs_counts = np.full(np.shape(obs_frequencies), sample_size)*obs_frequencies\n",
    "exp_counts = np.full(np.shape(exp_frequencies), sample_size)*exp_frequencies\n",
    "chi2stat, pval = stats.chisquare(f_obs=obs_counts,f_exp=exp_counts)\n",
    "print(\"p-value: \"+str(pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Such a low p-value indicate that the fit is bad (observed and expected counts are significantly different)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kolmogorov-Smirnov test\n",
    "\n",
    "* The KS test is a ___non-parametric method___ to test for GoF with continuous distributions/variables\n",
    "* `scipy.stats` offers methods for both 1-sample and 2-sample KS test\n",
    "    * `statsmodels` function `kstest_fit` (https://www.statsmodels.org/dev/generated/statsmodels.stats.diagnostic.kstest_fit.html) can be applied only when the a priori model is normal or exponential\n",
    "* for the 1-sample test, we will draw a sample from an exponential distribution, say $\\mathbf{x}_1$, and test fit with this model\n",
    "* for the 2-sample test, we will draw a sample from an exponential distribution, , say $\\mathbf{x}_2$, manipulate it, and compare it with $\\mathbf{x}_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# our a priori model\n",
    "priori_model = stats.expon(scale=1/10)\n",
    "\n",
    "# print pdf and cdf\n",
    "xs=np.linspace(priori_model.ppf(0.01),priori_model.ppf(0.99),100)\n",
    "plt.plot(xs,priori_model.pdf(xs))\n",
    "plt.title('pdf of exponential')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(xs,priori_model.cdf(xs))\n",
    "plt.title('cdf of exponential')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now draw our sample x1 from the model\n",
    "# use 40 obs\n",
    "n = 40\n",
    "x1 = priori_model.rvs(n)\n",
    "\n",
    "# let's plot the empirical CDF of our sample\n",
    "# and compare it with the model\n",
    "# note that we use np.mean(x1<=x), i.e., \n",
    "# the proportion of x1 observations <= x, i.e., \n",
    "# the empirical cdf\n",
    "ecdf_x1 = [np.mean(x1<=x) for x in xs]\n",
    "plt.plot(xs,priori_model.cdf(xs))\n",
    "# we use a step plot because it's discrete\n",
    "plt.step(xs,ecdf_x1)\n",
    "# or could have used seaborn ecdfplot, see https://seaborn.pydata.org/generated/seaborn.ecdfplot.html\n",
    "plt.title('ecdf vs cdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's perform the 1-sample ks test\n",
    "# stats.ks_1samp takes in input the one sample (argument x), the cdf function of our model (argument cdf), and the direction of the test\n",
    "# it returns the KS statistic and the p-value\n",
    "# see more at https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_1samp.html\n",
    "KS_stat, p_val = stats.ks_1samp(x=x1, cdf=priori_model.cdf)\n",
    "print(\"p-value: \"+str(p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected we obtain a high p-value, indicating validity of the null hypothesis (a priori distribution = distribution of our sample) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's generate a second sample, and manipulate it by adding some \"non-white\" noise\n",
    "x2 = priori_model.rvs(n)\n",
    "# manipulate it by adding white gaussian noise (mean 0 and std 0.5)\n",
    "x2 = x2 + stats.norm(loc=0,scale=0.5).rvs(n)\n",
    "\n",
    "# compare the ecdfs of x1 and x2\n",
    "# (x2 will have a different range of values than x1)\n",
    "xs2 = np.arange(np.min(x2), np.max(x2)+0.01, 0.01)\n",
    "ecdf_x2 = [np.mean(x2<=x) for x in xs2]\n",
    "#Â plot them\n",
    "plt.step(xs,ecdf_x1)\n",
    "plt.step(xs2,ecdf_x2)\n",
    "# or could have used seaborn ecdfplot, see https://seaborn.pydata.org/generated/seaborn.ecdfplot.html\n",
    "plt.title('ecdfs of $\\mathbf{x}_1$ vs $\\mathbf{x}_2$')\n",
    "plt.legend(['$\\mathbf{x}_1$','$\\mathbf{x}_2$'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now perform a 2-sample KS test (see more at https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.ks_2samp.html)\n",
    "_, p_val_2sample = stats.ks_2samp(x1, x2)\n",
    "print(\"p-value: \"+str(p_val_2sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now see that the p-value is much smaller, which strongly supports the alternative hypothesis of the two distributions being different (as expected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sign test\n",
    "* We saw the sign test to do inference about one-sample median and median of paired samples (to test if there are significant differences)\n",
    "* So it's the non-parametric equivalent of the one-sample T test / paired two-sample T test\n",
    "    * Recall, the null is that the true median is $m_0$\n",
    "    * the test statistic is the number of observations (or paired differences) above $m_0$\n",
    "    * the statistic distribution under the null is a binomial with $p=1/2$\n",
    "    * we saw how to adapt the test to other quantiles, but we skip this for now\n",
    "* We can easily implement our own sign test\n",
    "    * `scipy.stats` includes Wilcoxon signed-rank test, but this test makes more assumptions (see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.wilcoxon.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the function takes in input one sample (which could be paired differences),\n",
    "# the null value of the median, and returns the test statistic and p-value\n",
    "# by default applies a two-sided test and randomly resolves ties (see lectures)\n",
    "def sign_test(sample, m0=0):\n",
    "    n = sample.size\n",
    "    h0_distr = stats.binom(n=n,p=0.5)\n",
    "    num_wins = np.sum(sample>m0)\n",
    "    num_ties = np.sum(sample==m0)\n",
    "    # random tie resolution corresponds to drawing\n",
    "    # a coin (a 0.5 bernoulli) for each tie, i.e., \n",
    "    # drawing one binomial observation with parameters \n",
    "    # n=num_ties and p=0.5\n",
    "    stat_val = num_wins + stats.binom(n=num_ties, p=0.5).rvs(1)\n",
    "    left_tail_p = h0_distr.cdf(stat_val)\n",
    "    # as usual, for discrete vars, P(X>=x)=1-P(X<x))=1-P(X<=x-1) \n",
    "    right_tail_p = 1 - h0_distr.cdf(stat_val-1)\n",
    "    p_val=2*np.min([left_tail_p,right_tail_p])\n",
    "    return stat_val, p_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Let's look into the same example we used for the paired two-sample T test in Lab 4. \n",
    "* In CDC BRFSS survey dataset, we have, for each individual, two variables, weight and desired weight, to which we apply a (paired) two-sample sign test using our implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â load the CDC BRFSS survey dataset and take a random sample of 50 observations\n",
    "df_cdc = pd.read_csv(\"cdc.csv\")\n",
    "n_cdc = 50\n",
    "df_cdc_sample = df_cdc.sample(n_cdc)\n",
    "# compute the pairwise differences between weight and wtdesire\n",
    "w_diff = df_cdc_sample.weight-df_cdc_sample.wtdesire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply our sign test implementation with m0=0 (default)\n",
    "# i.e., to test the null hypothesis that it is equally probable to observe a positive and a negative difference\n",
    "stat_val, p_val = sign_test(w_diff)\n",
    "print(\"Test statistic value (num of positive differences with random tie correction): \"+str(stat_val))\n",
    "print(\"p-value: \"+str(p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strong evidence in favour of the alternative hypothesis that there is a difference between weight and desired weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Mann-Whitney U test allows us to test whether two independent samples have the same distribution or one stochastically dominates the other\n",
    "    * So it's the non-parametric equivalent of Welch's T test\n",
    "* `scipy.stats` includes function `mannwhitneyu`, which uses a normal approximation of the null distribution, see https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.mannwhitneyu.html\n",
    "* Let's repeat the same experiment as above, but this time, we sample from columns weight and desired weight independently, thereby obtaining two independent samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw independent samples of 50 observations each from weight and wtdesire columns\n",
    "n = 50\n",
    "weights = df_cdc.weight.sample(n)\n",
    "desired_weights = df_cdc.wtdesire.sample(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_stat, p_val = stats.mannwhitneyu(weights,desired_weights,alternative='two-sided')\n",
    "print(\"p-value (independent samples): \"+str(p_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We see that the p-value is much higher than in paired case, which would suggest absence of a difference between weight and desired weight\n",
    "* The reason is that by treating these observations as independent, we introduce a lot of inter-subject variability\n",
    "    * the variability of weights is very high, which makes it difficult to assess presence of an actual difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Bootstrap sampling allows us to \"simulate\" sampling from a population by resampling from your sample\n",
    "* In this way, we can create a proxy for the sampling distribution of any statistic and perform inference\n",
    "* In the example below, we assume that the data comes from a uniform distribution between 0 and 10, $\\mathcal{U}(0,10)$ (unknown true population distribution). What we know is only one iid sample from  the population. The statistic of interest $\\theta$ will be the interquartile range of the distribution (we could have chosen other statistics as well). And we will use the bootstrap to perform inference about $\\theta$. In particular, we will\n",
    "     * Derive the (unknown) sampling distribution of our statistic $\\hat{\\theta}$ (we derive an empirical approximation by sampling from the true unknown population and compute the statistic for each sample)\n",
    "     * And compare the above with the bootstrap sampling distribution of $\\tilde{\\theta}$ (obtained by applying the statistic to each resample of the given sample)\n",
    "     * Finally we will derive a confidence interval estimate for $\\theta$ using the ___empirical bootstrap CI___ method:  $$\\left[2\\hat{\\theta}(x) - \\tilde{\\theta}_{\\left((1-\\alpha/2)\\cdot B\\right)}, 2\\hat{\\theta}(x) + \\tilde{\\theta}_{\\left(\\alpha/2\\cdot B\\right)} \\right]$$ (see lecture slides)\n",
    "     * and we will compute p-values using the ___shifting method___ (i.e., generating a null distribution by shifting the bootstrap sampling distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the unknown population distribution, \n",
    "# consider a uniform distribution with endpoints 0 and 10\n",
    "pop_distr = stats.uniform(scale=10)\n",
    "# plot the pdf of the distribution\n",
    "x = np.linspace(pop_distr.ppf(0.001),pop_distr.ppf(0.999), 1000)\n",
    "plt.plot(x, pop_distr.pdf(x), '--')\n",
    "plt.title('Population distribution')\n",
    "plt.show()\n",
    "\n",
    "# take a sample from the population\n",
    "# this will be our fixed, known, sample that we use for bootstrapping\n",
    "sample_size = 200\n",
    "sample = pop_distr.rvs(sample_size)\n",
    "# plot the sample with seaborn displot\n",
    "sns.histplot(sample,stat='probability')\n",
    "plt.title('Sample $x$')\n",
    "plt.show()\n",
    "\n",
    "# our statistic is the IQR\n",
    "# let's define a function handle for that\n",
    "my_stat = lambda sample : stats.iqr(sample)\n",
    "\n",
    "# note that the true IQR of U(0,10) is of course 5 \n",
    "# (because the 75-th percentile is 7.5 and 25-th percentile is 2.5)\n",
    "\n",
    "# this is our sample estimate of the statistic\n",
    "theta_hat_est = my_stat(sample)\n",
    "print('Estimate of statistic on \"sample\" is: '+str(theta_hat_est))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now derive (an empirical approximation of) the true sampling distribution of our statistic\n",
    "# by drawing fresh samples from the population distribution \n",
    "# (note that we can't do this in reality, here is just for the sake of the example)\n",
    "\n",
    "# let's draw 2000 samples\n",
    "sample_num = 2000\n",
    "\n",
    "# initialize a numpy array of size (sample_num, 1) to store the sample statistics\n",
    "theta_hats = np.empty([sample_num, 1])\n",
    "\n",
    "# draw the samples by calling the rvs function\n",
    "for i in range(sample_num):\n",
    "    # apply our statistic function to a random sample drawn from the pop distr\n",
    "    theta_hats[i] = my_stat(pop_distr.rvs(sample_size))\n",
    "    \n",
    "    \n",
    "#Â plot the sampling distribution\n",
    "sns.histplot(theta_hats, stat='probability')\n",
    "plt.title('True sampling distribution of $\\widehat{\\\\theta}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's compute the bootstrap proxy of the sampling distribution\n",
    "# by resampling our sample (the only thing we know)\n",
    "\n",
    "# let's define a useful function handle for resampling\n",
    "# rng is numpy's random number generator\n",
    "rng = np.random.default_rng()\n",
    "resample = lambda sample, n : rng.choice(a=sample,size=n)\n",
    "\n",
    "# draw bootstrap samples, and store corresponding statistic values\n",
    "theta_tildes = np.empty([sample_num, 1])\n",
    "\n",
    "for i in range(sample_num):\n",
    "    # apply our statistic function to a bootstrap resample drawn from the given sample\n",
    "    theta_tildes[i] = my_stat(resample(sample=sample, n=sample_size))\n",
    "\n",
    "# plot the bootstrap sampling distribution\n",
    "sns.histplot(theta_tildes, stat='probability')\n",
    "plt.title('Bootstrap sampling distribution of $\\\\tilde{\\\\theta}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap confidence intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's derive a 99%-CI estimate with the empirical bootstrap method\n",
    "alpha = 0.01\n",
    "CI_left = 2*theta_hat_est - np.percentile(a=theta_tildes,q=100*(1-alpha/2))\n",
    "CI_right = 2*theta_hat_est + np.percentile(a=theta_tildes,q=100*(alpha/2))\n",
    "print(\"99%-CI estimate for the IQR (empirical bootstrap): \"+str([CI_left,CI_right]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the interval is quite imprecise (large). In some cases, like this one, the percentile bootstrap method might work better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# percentile CI method\n",
    "# recall, we just pick the percentiles of the bootstrap distributions that cover  the central 1-\\alpha probability\n",
    "CI_perc = np.percentile(a=theta_tildes,q=[100*(alpha/2), 100*(1-alpha/2)])\n",
    "print(\"99%-CI estimate for the IQR (percentile bootstrap): \"+str(CI_perc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bootstrap p-values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Recall that we can \"simulate\" the unknown null distribution by shifting the bootstrap distribution by $\\theta_0+\\hat{\\theta}(x)$, where $\\theta_0$ is the null value and $\\hat{\\theta}(x)$ is the estimate of the statistic on our sample $x$\n",
    "    * and with the null distribution, we can compute p-values\n",
    "* We will compute a two-sided p-value for the null hypothesis that the true value of the statistic is 5 (which we know it's the true IQR value)\n",
    "    * we expect a high p-value, so we shouldn't be able to reject the null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# null value\n",
    "theta_0 = 5\n",
    "# let's simulate the null distribution by shifting our bootstrap sampling distribution\n",
    "bootstrap_null = theta_tildes + (theta_0-theta_hat_est)\n",
    "# plot it\n",
    "sns.histplot(bootstrap_null, stat='probability')\n",
    "plt.title('Bootstrap null distribution')\n",
    "plt.show()\n",
    "\n",
    "# compute a two-sided p-value\n",
    "# we derive the p-values for each side first (with the +1 correction)\n",
    "left_pval = (np.sum(bootstrap_null<=theta_hat_est)+1)/(bootstrap_null.size + 1)\n",
    "right_pval = (np.sum(bootstrap_null>=theta_hat_est)+1)/(bootstrap_null.size + 1)\n",
    "\n",
    "bootstrap_pval = 2*np.min([left_pval,right_pval])\n",
    "print(\"bootstrap two-sided p-value is: \"+str(bootstrap_pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The obtained p-value is very high, which supports the null hypothesis that 5 is the true value of our statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
