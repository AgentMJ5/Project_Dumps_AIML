{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Syntactic Parsing with Spacy \n",
    "Spacy provides a full-stack syntactic analysis toolkit. In this exercise, we will first take a look at how to use Spacy to perform different syntactic analysis tasks, and then try to use the analysis results to answer some questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load() # this is an object that will process your text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\tPenn\tUniversal\n",
      "He\tPRP\tPRON\n",
      "went\tVBD\tVERB\n",
      "to\tIN\tADP\n",
      "South\tNNP\tPROPN\n",
      "Africa\tNNP\tPROPN\n",
      "for\tIN\tADP\n",
      "holiday\tNN\tNOUN\n",
      ".\t.\tPUNCT\n"
     ]
    }
   ],
   "source": [
    "# POS tagger\n",
    "\n",
    "# make up (or cut and paste from web pages) different sentences here. \n",
    "doc = nlp('He went to South Africa for holiday.')\n",
    "print('word\\tPenn\\tUniversal')\n",
    "for ww in doc:\n",
    "    print('{}\\t{}\\t{}'.format(ww,ww.tag_,ww.pos_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package tagsets to C:\\Users\\jathin\n",
      "[nltk_data]     varma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping help\\tagsets.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get help information for pos tags from nltk, which has a convenient help\n",
    "# function\n",
    "import nltk\n",
    "\n",
    "# comment out the next line if you have already downloaded tagsets\n",
    "nltk.download('tagsets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NNP: noun, proper, singular\n",
      "    Motown Venneboerger Czestochwa Ranzer Conchita Trumplane Christos\n",
      "    Oceanside Escobar Kreisler Sawyer Cougar Yvette Ervin ODI Darryl CTCA\n",
      "    Shannon A.K.C. Meltex Liverpool ...\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# the nltk function allows you to print out definitions of each tag above, \n",
    "# and to find examples of words with this tag. \n",
    "# figure out what the different tags mean. \n",
    "print(nltk.help.upenn_tagset('NNP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking\n",
    "\n",
    "Try out chunking on many individual sentences and larger bodies of text from the web. (Save these to .txt files, and load them in as python strings using the read or readlines functions.) \n",
    "\n",
    "Do you agree with the chunks? Can you find cases where the chunking fails? Roughly what is the failure rate?  In a case where the chunking fails, try variations of the sentence to try to understand why the failure might occur. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A tram\n",
      "St Peters Square\n",
      "Manchester\n",
      "Photograph\n",
      "Christopher Thomond\n"
     ]
    }
   ],
   "source": [
    "# chunking\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "doc = nlp('A tram in St Peters Square, Manchester. Photograph: Christopher Thomond')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition \n",
    "There is a deceptively simple spacy pipeline for this complex process! \n",
    "\n",
    "Try named entity recognition for some sentences that you make up: does it work? \n",
    "\n",
    "Next, cut and paste some text from a newspaper website into a .txt document, and then load it into Python. Run Spacy on it, and see how many of the named entities are correctly identified.  Can you find cases where it makes mistakes? \n",
    "\n",
    "How could you use NER in applications? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "named entity 0: AI, label ORG\n",
      "named entity 1: up to $15 trillion, label MONEY\n",
      "named entity 2: 2030, label DATE\n",
      "named entity 3: PwC, label WORK_OF_ART\n"
     ]
    }
   ],
   "source": [
    "# now using Spacy\n",
    "# named entity recognition (NER)\n",
    "nlp = en_core_web_sm.load()\n",
    "doc = nlp('AI, robotics and other forms of smart automation have the potential to bring great economic benefits, contributing up to $15 trillion to global GDP by 2030 according to PwC analysis. This extra wealth will also generate the demand for many jobs, but there are also concerns that it could displace many existing jobs.')\n",
    "\n",
    "for i,ent in enumerate(doc.ents):\n",
    "    print('named entity {}: {}, label {}'.format(i,ent.text,ent.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    AI\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", robotics and other forms of smart automation have the potential to bring great economic benefits, contributing \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    up to $15 trillion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       " to global GDP by \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    2030\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " according to \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    PwC\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       " analysis. This extra wealth will also generate the demand for many jobs, but there are also concerns that it could displace many existing jobs.</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualization of NER results\n",
    "from spacy import displacy\n",
    "displacy.render(doc,style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependency parsing\n",
    "\n",
    "Use the Spacy dependency parser.  Try it out on many different sentences (one sentence at a time!) and try to understand the parse produced. \n",
    "\n",
    "Does the parse correctly display the structure of the sentence? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some sentences to compare. (It is easier to find structural differences when you compare different related sentences with different structures.) \n",
    "\n",
    "Compare:\n",
    "\n",
    "A:  'I shot the elephant wearing my pajamas' \n",
    "\n",
    "B:  'I shot the elephant eating my tree'\n",
    "\n",
    "In A, I am wearing my pajamas, and this phrase is connected to the subject of the sentence.  In B, the elephant is eating my tree, and this phrase depends on the object (the elephant).  There is evidently sufficient \"background knowledge\" somewhere in the parser to distinguish these possibilities. Try to make up other sentences to find out how the parser is detecting the correct (or incorrect) structure. \n",
    "\n",
    "Compare\n",
    "\n",
    "C: 'John loves Mary, Mary loves John'\n",
    "\n",
    "D: 'John loves Mary and Mary loves John' \n",
    "\n",
    "The parse of C shows two sentences side by side, joined by a ccomp dependency, which is correct. You can find out what the dependencies mean at universaldependencies.org  --- do this! Look them up! \n",
    "\n",
    "In D, something goes horribly wrong: the parser misinterprets the sentence. \n",
    "\n",
    "Now make up pairs of sentences of your own (similar sentences with different grammatical structure) and see if the parser recognises them. \n",
    "\n",
    "Cut and paste sentences from the web (news stories, fiction, tweets, etc) and see if you agree with Spacy's parsed structures. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John nsubj loves VERB []\n",
      "loves ccomp loves VERB [John, Mary]\n",
      "Mary dobj loves VERB []\n",
      ", punct loves VERB []\n",
      "Mary nsubj loves VERB []\n",
      "loves ROOT loves VERB [loves, ,, Mary, John]\n",
      "John dobj loves VERB []\n"
     ]
    }
   ],
   "source": [
    "# dependency parsing \n",
    "\n",
    "doc = nlp('John loves Mary, Mary loves John')\n",
    "for token in doc:\n",
    "    print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])\n",
    "\n",
    "# the output below will be a little obscure: \n",
    "# the tree visualisation in the next cell is a lot clearer, but perhaps\n",
    "# only for relatively short sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"48f26749b41a48d287bfcb6fe01ef68e-0\" class=\"displacy\" width=\"1100\" height=\"312.0\" direction=\"ltr\" style=\"max-width: none; height: 312.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">John</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">loves</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">Mary,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">Mary</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">loves</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"222.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">John</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-48f26749b41a48d287bfcb6fe01ef68e-0-0\" stroke-width=\"2px\" d=\"M70,177.0 C70,89.5 220.0,89.5 220.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-48f26749b41a48d287bfcb6fe01ef68e-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,179.0 L62,167.0 78,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-48f26749b41a48d287bfcb6fe01ef68e-0-1\" stroke-width=\"2px\" d=\"M245,177.0 C245,2.0 750.0,2.0 750.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-48f26749b41a48d287bfcb6fe01ef68e-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">ccomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,179.0 L237,167.0 253,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-48f26749b41a48d287bfcb6fe01ef68e-0-2\" stroke-width=\"2px\" d=\"M245,177.0 C245,89.5 395.0,89.5 395.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-48f26749b41a48d287bfcb6fe01ef68e-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M395.0,179.0 L403.0,167.0 387.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-48f26749b41a48d287bfcb6fe01ef68e-0-3\" stroke-width=\"2px\" d=\"M595,177.0 C595,89.5 745.0,89.5 745.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-48f26749b41a48d287bfcb6fe01ef68e-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,179.0 L587,167.0 603,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-48f26749b41a48d287bfcb6fe01ef68e-0-4\" stroke-width=\"2px\" d=\"M770,177.0 C770,89.5 920.0,89.5 920.0,177.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-48f26749b41a48d287bfcb6fe01ef68e-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M920.0,179.0 L928.0,167.0 912.0,167.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize the dependency parsing result\n",
    "# Obviously only do this for one sentence at a time! \n",
    "from spacy import displacy\n",
    "displacy.render(doc,style='dep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon|Amazon|nsubj|purchases\n",
      "Whole Foods|Foods|dobj|purchases\n"
     ]
    }
   ],
   "source": [
    "# chunks also includes dependency information\n",
    "# look at the Spacy documentation for chunks\n",
    "\n",
    "doc = nlp('Amazon purchases Whole Foods for $13.4 billion.')\n",
    "for chunk in doc.noun_chunks:\n",
    "    print('{}|{}|{}|{}'.format(chunk.text, chunk.root.text, chunk.root.dep_,\n",
    "            chunk.root.head.text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A tiny application\n",
    "\n",
    "The next cell shows one way to use dependency parsing to answer a semantic question: to find instances of one entity purchasing another.  Do you think this is an extendable approach?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon bought Whole Foods\n"
     ]
    }
   ],
   "source": [
    "# A simple application of chunking and dependency parsing\n",
    "def who_purchases_whom(doc):\n",
    "    for chunk in doc.noun_chunks:\n",
    "        if 'purchase' in chunk.root.head.text and 'subj' in chunk.root.dep_:\n",
    "            subj = chunk.text\n",
    "        elif 'purchase' in chunk.root.head.text and 'obj' in chunk.root.dep_:\n",
    "            obj = chunk.text\n",
    "    return subj, obj\n",
    "\n",
    "subj, obj = who_purchases_whom(doc)\n",
    "print('{} bought {}'.format(subj,obj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
